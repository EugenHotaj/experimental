{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaTUVKFq019FhG4tYYFQSZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EugenHotaj/cML/blob/main/cuda_vec_addition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "lWW6UpDAHHqy"
      },
      "outputs": [],
      "source": [
        "# Goofy way to use free GPUs from Google. Write all our CUDA code as a Python string,\n",
        "# write that to a file, compile with NVCC, then run.\n",
        "\n",
        "code = r\"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <assert.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define N 10000000\n",
        "#define MAX_ERR 1e-6\n",
        "\n",
        "__global__ void vector_add(float *out, float *a, float *b, int n) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < n) {\n",
        "        out[i] = a[i] + b[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    float *a_h, *b_h, *out_h;\n",
        "    float *a_d, *b_d, *out_d;\n",
        "    int size_bytes = sizeof(float) * N;\n",
        "\n",
        "    // Allocate host memory.\n",
        "    a_h   = (float*)malloc(size_bytes);\n",
        "    b_h   = (float*)malloc(size_bytes);\n",
        "    out_h = (float*)malloc(size_bytes);\n",
        "\n",
        "    // Initialize host arrays.\n",
        "    for(int i = 0; i < N; i++){\n",
        "        a_h[i] = 1.0f;\n",
        "        b_h[i] = 2.0f;\n",
        "    }\n",
        "\n",
        "    // Allocate device memory.\n",
        "    cudaMalloc((void**)&a_d, size_bytes);\n",
        "    cudaMalloc((void**)&b_d, size_bytes);\n",
        "    cudaMalloc((void**)&out_d, size_bytes);\n",
        "\n",
        "    // Transfer data from host to device memory.\n",
        "    cudaMemcpy(a_d, a_h, size_bytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(b_d, b_h, size_bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Execute kernel.\n",
        "    vector_add<<<ceil(N/256.0),256>>>(out_d, a_d, b_d, N);\n",
        "\n",
        "    // Transfer data back to host memory.\n",
        "    cudaMemcpy(out_h, out_d, size_bytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Verification.\n",
        "    for(int i = 0; i < N; i++){\n",
        "        assert(fabs(out_h[i] - a_h[i] - b_h[i]) < MAX_ERR);\n",
        "    }\n",
        "    printf(\"out[0] = %f\\n\", out_h[0]);\n",
        "    printf(\"PASSED\\n\");\n",
        "\n",
        "    // Deallocate device memory.\n",
        "    cudaFree(a_d);\n",
        "    cudaFree(b_d);\n",
        "    cudaFree(out_d);\n",
        "\n",
        "    // Deallocate host memory.\n",
        "    free(a_h);\n",
        "    free(b_h);\n",
        "    free(out_h);\n",
        "}\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"code.cu\", \"w\") as file_:\n",
        "    file_.write(code)\n",
        "!nvcc code.cu && ./a.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGBWI8PDKOnu",
        "outputId": "282de04e-ffdb-443b-c70d-9c230f98e62f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out[0] = 3.000000\n",
            "PASSED\n"
          ]
        }
      ]
    }
  ]
}